{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Примеры работы с векторными представлениями слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для экспериментов будем использовать библиотеку [gensim.](https://radimrehurek.com/gensim/auto_examples/index.html#documentation)\n",
    "\n",
    "Используем предобученную модель с сайта [RusVectores.](https://rusvectores.org/ru/models/)\n",
    "\n",
    "Необходимо отметить, что в существующих универсальных библиотеках (spacy, natasha) также реализованы возможности для работы с векторными представлениями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'corpus/ruscorpora_mystem_cbow_300_2_2015.bin.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выдаем ближайшие слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['король_S', 'королева_S', 'мужчина_S', 'женщина_S']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "СЛОВО - король_S\n",
      "5 ближайших соседей слова:\n",
      "принц_S => 0.672407865524292\n",
      "герцог_S => 0.6092391610145569\n",
      "королева_S => 0.5848619937896729\n",
      "император_S => 0.5664135813713074\n",
      "курфюрст_S => 0.5498137474060059\n",
      "\n",
      "СЛОВО - королева_S\n",
      "5 ближайших соседей слова:\n",
      "король_S => 0.5848619937896729\n",
      "принцесса_S => 0.5745126008987427\n",
      "герцогиня_S => 0.5264037847518921\n",
      "принц_S => 0.4866287410259247\n",
      "герцог_S => 0.4809737503528595\n",
      "\n",
      "СЛОВО - мужчина_S\n",
      "5 ближайших соседей слова:\n",
      "женщина_S => 0.7638137340545654\n",
      "девушка_S => 0.6010492444038391\n",
      "вамп_A => 0.571452260017395\n",
      "девица_S => 0.5431127548217773\n",
      "немолодой_A => 0.5392476320266724\n",
      "\n",
      "СЛОВО - женщина_S\n",
      "5 ближайших соседей слова:\n",
      "девушка_S => 0.7725452184677124\n",
      "мужчина_S => 0.7638137340545654\n",
      "девица_S => 0.6532836556434631\n",
      "дама_S => 0.6249092817306519\n",
      "человек_S => 0.5979774594306946\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    if word in model:\n",
    "        print('\\nСЛОВО - {}'.format(word))\n",
    "        print('5 ближайших соседей слова:')\n",
    "        for word, sim in model.most_similar(positive=[word], topn=5):\n",
    "            print('{} => {}'.format(word, sim))\n",
    "    else:\n",
    "        print('Слово \"{}\" не найдено в модели'.format(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Находим близость между словами и строим аналогии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.584862\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('король_S', 'королева_S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76381373\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('мужчина_S', 'женщина_S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('принц_S', 0.5996884107589722), ('герцог_S', 0.5775512456893921), ('королева_S', 0.5213630199432373), ('принцесса_S', 0.5140257477760315), ('герцогиня_S', 0.5112662315368652), ('император_S', 0.5007482171058655), ('посланник_S', 0.4788142442703247), ('посол_S', 0.453449547290802), ('царь_S', 0.4492655396461487), ('императрица_S', 0.4435415267944336)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['король_S', 'женщина_S'], negative=['мужчина_S']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4549625\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('франция_S', 'париж_S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39409316\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('испания_S', 'мадрид_S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('лондон_S', 0.6315737366676331), ('берлин_S', 0.6051103472709656), ('мюнхен_S', 0.5779414176940918), ('нью-йорк_S', 0.5686920881271362), ('москва_S', 0.5660902261734009), ('амстердам_S', 0.5587795376777649), ('милан_S', 0.5564035773277283), ('венеция_S', 0.5518475770950317), ('авиньон_S', 0.5482670068740845), ('копенгаген_S', 0.539797842502594)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['париж_S', 'испания_S'], negative=['франция_S']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучим word2vec на наборе данных imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Paladin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  value\n",
       "0  A very, very, very slow-moving, aimless movie ...      0\n",
       "1  Not sure who was more lost - the flat characte...      0\n",
       "2  Attempting artiness with black & white and cle...      0\n",
       "3       Very little music or anything to speak of.        0\n",
       "4  The best scene in the movie was when Gerardo i...      1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузка данных\n",
    "imdb_df = pd.read_csv(\"data/imdb_labelled.txt\", delimiter='\\t', header=None, names=['text', 'value'])\n",
    "imdb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовим корпус\n",
    "corpus = []\n",
    "stop_words = stopwords.words('english')\n",
    "tok = WordPunctTokenizer()\n",
    "for line in imdb_df['text'].values:\n",
    "    line1 = line.strip().lower()\n",
    "    line1 = re.sub(\"[^a-zA-Z]\",\" \", line1)\n",
    "    text_tok = tok.tokenize(line1)\n",
    "    text_tok1 = [w for w in text_tok if not w in stop_words]\n",
    "    corpus.append(text_tok1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['slow',\n",
       "  'moving',\n",
       "  'aimless',\n",
       "  'movie',\n",
       "  'distressed',\n",
       "  'drifting',\n",
       "  'young',\n",
       "  'man'],\n",
       " ['sure',\n",
       "  'lost',\n",
       "  'flat',\n",
       "  'characters',\n",
       "  'audience',\n",
       "  'nearly',\n",
       "  'half',\n",
       "  'walked'],\n",
       " ['attempting',\n",
       "  'artiness',\n",
       "  'black',\n",
       "  'white',\n",
       "  'clever',\n",
       "  'camera',\n",
       "  'angles',\n",
       "  'movie',\n",
       "  'disappointed',\n",
       "  'became',\n",
       "  'even',\n",
       "  'ridiculous',\n",
       "  'acting',\n",
       "  'poor',\n",
       "  'plot',\n",
       "  'lines',\n",
       "  'almost',\n",
       "  'non',\n",
       "  'existent'],\n",
       " ['little', 'music', 'anything', 'speak'],\n",
       " ['best',\n",
       "  'scene',\n",
       "  'movie',\n",
       "  'gerardo',\n",
       "  'trying',\n",
       "  'find',\n",
       "  'song',\n",
       "  'keeps',\n",
       "  'running',\n",
       "  'head']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# количество текстов в корпусе не изменилось и соответствует целевому признаку\n",
    "assert imdb_df.shape[0]==len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 37.9 ms\n"
     ]
    }
   ],
   "source": [
    "%time model_imdb = word2vec.Word2Vec(corpus, workers=4, min_count=10, window=10, sample=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ever', 0.34744006395339966), ('watching', 0.31204521656036377), ('predictable', 0.28830358386039734), ('acting', 0.2815362811088562), ('really', 0.27776145935058594)]\n"
     ]
    }
   ],
   "source": [
    "# Проверим, что модель обучилась\n",
    "print(model_imdb.wv.most_similar(positive=['find'], topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Решение задачи анализа тональности текста на основе модели word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(v, c):\n",
    "    model = Pipeline(\n",
    "        [(\"vectorizer\", v), \n",
    "         (\"classifier\", c)])\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print_accuracy_score_for_classes(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingVectorizer(object):\n",
    "    '''\n",
    "    Для текста усредним вектора входящих в него слов\n",
    "    '''\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.size = model.vector_size\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([np.mean(\n",
    "            [self.model[w] for w in words if w in self.model] \n",
    "            or [np.zeros(self.size)], axis=0)\n",
    "            for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray) -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Вычисление метрики accuracy для каждого класса\n",
    "    y_true - истинные значения классов\n",
    "    y_pred - предсказанные значения классов\n",
    "    Возвращает словарь: ключ - метка класса, \n",
    "    значение - Accuracy для данного класса\n",
    "    \"\"\"\n",
    "    # Для удобства фильтрации сформируем Pandas DataFrame \n",
    "    d = {'t': y_true, 'p': y_pred}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    # Метки классов\n",
    "    classes = np.unique(y_true)\n",
    "    # Результирующий словарь\n",
    "    res = dict()\n",
    "    # Перебор меток классов\n",
    "    for c in classes:\n",
    "        # отфильтруем данные, которые соответствуют \n",
    "        # текущей метке класса в истинных значениях\n",
    "        temp_data_flt = df[df['t']==c]\n",
    "        # расчет accuracy для заданной метки класса\n",
    "        temp_acc = accuracy_score(\n",
    "            temp_data_flt['t'].values, \n",
    "            temp_data_flt['p'].values)\n",
    "        # сохранение результата в словарь\n",
    "        res[c] = temp_acc\n",
    "    return res\n",
    "\n",
    "def print_accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Вывод метрики accuracy для каждого класса\n",
    "    \"\"\"\n",
    "    accs = accuracy_score_for_classes(y_true, y_pred)\n",
    "    if len(accs)>0:\n",
    "        print('Метка \\t Accuracy')\n",
    "    for i in accs:\n",
    "        print('{} \\t {}'.format(i, accs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучающая и тестовая выборки\n",
    "boundary = 700\n",
    "X_train = corpus[:boundary] \n",
    "X_test = corpus[boundary:]\n",
    "y_train = imdb_df.value.values[:boundary]\n",
    "y_test = imdb_df.value.values[boundary:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 0.46153846153846156\n",
      "1 \t 0.6\n"
     ]
    }
   ],
   "source": [
    "sentiment(EmbeddingVectorizer(model_imdb.wv), LogisticRegression(C=5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
